{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4b5792-c7d8-4ba4-bcb5-6857d32c71ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Peak Tester\n",
    "==================================\n",
    "\n",
    "A statistical test to determine if an emissions series has peaked\n",
    "based on N years of declining emissions data, and accounting for the\n",
    "noise in historical data (which is assumed to continue)\n",
    "\n",
    "Usage:\n",
    "    # Initialize the test\n",
    "    peak_test = EmissionsPeakTest()\n",
    "    \n",
    "    # Load your data\n",
    "    peak_test.load_historical_data('path/to/your/data.csv')  # or use simulate_data()\n",
    "    \n",
    "    # Characterize noise\n",
    "    peak_test.characterize_noise(method='all_data')  # or 'segments'\n",
    "    \n",
    "    # Set test data\n",
    "    test_data = [(2021, 36500), (2022, 35800), (2023, 35100)]\n",
    "    peak_test.set_test_data(test_data)\n",
    "    \n",
    "    # Run the test\n",
    "    results = peak_test.run_bootstrap_test(n_bootstrap=1000)\n",
    "    \n",
    "    # Visualize results\n",
    "    peak_test.plot_analysis()\n",
    "    \n",
    "    # Get interpretation\n",
    "    interpretation = peak_test.interpret_results()\n",
    "\n",
    "Author: Neil Grant and Claire Fyson\n",
    "\"\"\"\n",
    "\n",
    "# Enhanced Emissions Peak Test\n",
    "# Improvements:\n",
    "# 1. Dynamic segment length optimization\n",
    "# 2. Autocorrelation handling\n",
    "# 3. Recent historical trend null hypothesis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from typing import List, Tuple, Dict, Optional, Callable, Union\n",
    "import warnings\n",
    "from itertools import combinations\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class EnhancedEmissionsPeakTest:\n",
    "    \"\"\"\n",
    "    Enhanced emissions peak test with variable segments and autocorrelation handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.historical_data: Optional[pd.DataFrame] = None\n",
    "        self.test_data: Optional[pd.DataFrame] = None\n",
    "        self.noise_params: Optional[Dict] = None\n",
    "        self.noise_generator: Optional[Callable] = None\n",
    "        self.bootstrap_results: Optional[Dict] = None\n",
    "        self.residuals: Optional[pd.Series] = None\n",
    "        self.trend_info: Optional[Dict] = None\n",
    "        self.autocorr_results: Optional[Dict] = None\n",
    "        self.optimal_segments: Optional[Dict] = None\n",
    "        \n",
    "    def load_historical_data(self, data_source: Union[str, pd.DataFrame], \n",
    "                           year_col: str = \"year\", emissions_col: str = \"emissions\",\n",
    "                           year_range: range = range(1970, 2020)) -> \"EnhancedEmissionsPeakTest\":\n",
    "        \"\"\"Load historical emissions data.\"\"\"\n",
    "        if isinstance(data_source, pd.DataFrame):\n",
    "            data = data_source.copy()\n",
    "        else:\n",
    "            raise ValueError(\"For this demo, please provide DataFrame directly\")\n",
    "            \n",
    "        # Standardize and filter\n",
    "        data = (data[[year_col, emissions_col]]\n",
    "                .rename(columns={year_col: \"year\", emissions_col: \"emissions\"})\n",
    "                .sort_values(\"year\").reset_index(drop=True))\n",
    "        \n",
    "        self.historical_data = data.loc[data[\"year\"].isin(year_range)]\n",
    "        self._validate_historical_data()\n",
    "        \n",
    "        print(f\"Loaded historical data: {self.historical_data['year'].min()}-{self.historical_data['year'].max()}\")\n",
    "        print(f\"Data points: {len(self.historical_data)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _validate_historical_data(self) -> None:\n",
    "        \"\"\"Validate the loaded historical data.\"\"\"\n",
    "        if self.historical_data is None:\n",
    "            raise ValueError(\"No historical data loaded\")\n",
    "        if len(self.historical_data) < 15:\n",
    "            raise ValueError(\"Need at least 15 years of historical data for robust analysis\")\n",
    "        if self.historical_data[\"emissions\"].isna().any():\n",
    "            raise ValueError(\"Historical data contains missing values\")\n",
    "    \n",
    "    def optimize_segment_lengths(self, min_segment: int = 5, max_segment: int = 20, \n",
    "                                max_segments: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Find optimal variable segment lengths by minimizing within-segment residual variance\n",
    "        while ensuring adequate sample size for noise characterization.\n",
    "        \"\"\"\n",
    "        years = self.historical_data[\"year\"].values\n",
    "        emissions = self.historical_data[\"emissions\"].values\n",
    "        n_years = len(years)\n",
    "        \n",
    "        print(\"Optimizing segment lengths...\")\n",
    "        \n",
    "        # Try different segmentation approaches\n",
    "        best_segmentation = None\n",
    "        best_score = float('inf')\n",
    "        segmentations_tested = []\n",
    "        \n",
    "        # Method 1: Equal length segments of varying sizes\n",
    "        for seg_len in range(min_segment, min(max_segment + 1, n_years // 2)):\n",
    "            segments = self._create_equal_segments(years, emissions, seg_len)\n",
    "            if len(segments) > max_segments:\n",
    "                continue\n",
    "                \n",
    "            score = self._evaluate_segmentation(segments)\n",
    "            segmentations_tested.append({\n",
    "                'method': 'equal',\n",
    "                'segment_length': seg_len,\n",
    "                'n_segments': len(segments),\n",
    "                'score': score,\n",
    "                'segments': segments\n",
    "            })\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_segmentation = segmentations_tested[-1]\n",
    "        \n",
    "        # Method 2: Dynamic segmentation based on structural breaks\n",
    "        dynamic_segments = self._create_dynamic_segments(years, emissions, min_segment)\n",
    "        if len(dynamic_segments) <= max_segments:\n",
    "            score = self._evaluate_segmentation(dynamic_segments)\n",
    "            segmentations_tested.append({\n",
    "                'method': 'dynamic',\n",
    "                'n_segments': len(dynamic_segments),\n",
    "                'score': score,\n",
    "                'segments': dynamic_segments\n",
    "            })\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_segmentation = segmentations_tested[-1]\n",
    "        \n",
    "        # Method 3: Overlapping segments (for comparison)\n",
    "        overlap_segments = self._create_overlapping_segments(years, emissions, \n",
    "                                                            segment_length=12, overlap=4)\n",
    "        if len(overlap_segments) <= max_segments:\n",
    "            score = self._evaluate_segmentation(overlap_segments)\n",
    "            segmentations_tested.append({\n",
    "                'method': 'overlapping',\n",
    "                'segment_length': 12,\n",
    "                'overlap': 4,\n",
    "                'n_segments': len(overlap_segments),\n",
    "                'score': score,\n",
    "                'segments': overlap_segments\n",
    "            })\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_segmentation = segmentations_tested[-1]\n",
    "        \n",
    "        self.optimal_segments = {\n",
    "            'best': best_segmentation,\n",
    "            'all_tested': segmentations_tested,\n",
    "            'selection_criteria': 'minimum_pooled_residual_variance'\n",
    "        }\n",
    "        \n",
    "        print(f\"Optimal segmentation: {best_segmentation['method']} method\")\n",
    "        print(f\"Number of segments: {best_segmentation['n_segments']}\")\n",
    "        print(f\"Score (residual variance): {best_score:.2f}\")\n",
    "        \n",
    "        return self.optimal_segments\n",
    "    \n",
    "    def _create_equal_segments(self, years: np.ndarray, emissions: np.ndarray, \n",
    "                              segment_length: int) -> List[Dict]:\n",
    "        \"\"\"Create equal-length segments.\"\"\"\n",
    "        segments = []\n",
    "        for i in range(0, len(years), segment_length):\n",
    "            end_idx = min(i + segment_length, len(years))\n",
    "            if end_idx - i >= 3:  # Need at least 3 points for regression\n",
    "                segments.append({\n",
    "                    'years': years[i:end_idx],\n",
    "                    'emissions': emissions[i:end_idx],\n",
    "                    'start_year': years[i],\n",
    "                    'end_year': years[end_idx-1]\n",
    "                })\n",
    "        return segments\n",
    "    \n",
    "    def _create_dynamic_segments(self, years: np.ndarray, emissions: np.ndarray,\n",
    "                                min_segment: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Create variable-length segments based on structural breaks detected\n",
    "        through rolling R² analysis.\n",
    "        \"\"\"\n",
    "        segments = []\n",
    "        start_idx = 0\n",
    "        \n",
    "        while start_idx < len(years):\n",
    "            best_end_idx = start_idx + min_segment\n",
    "            best_r2 = 0\n",
    "            \n",
    "            # Find the segment length that maximizes R² for this starting point\n",
    "            for end_idx in range(start_idx + min_segment, len(years) + 1):\n",
    "                if end_idx - start_idx > 25:  # Don't make segments too long\n",
    "                    break\n",
    "                    \n",
    "                X = years[start_idx:end_idx].reshape(-1, 1)\n",
    "                y = emissions[start_idx:end_idx]\n",
    "                \n",
    "                if len(X) >= 3:\n",
    "                    model = LinearRegression()\n",
    "                    model.fit(X, y)\n",
    "                    r2 = model.score(X, y)\n",
    "                    \n",
    "                    if r2 > best_r2:\n",
    "                        best_r2 = r2\n",
    "                        best_end_idx = end_idx\n",
    "            \n",
    "            # Create segment\n",
    "            segments.append({\n",
    "                'years': years[start_idx:best_end_idx],\n",
    "                'emissions': emissions[start_idx:best_end_idx],\n",
    "                'start_year': years[start_idx],\n",
    "                'end_year': years[best_end_idx-1],\n",
    "                'r2': best_r2\n",
    "            })\n",
    "            \n",
    "            start_idx = best_end_idx\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _create_overlapping_segments(self, years: np.ndarray, emissions: np.ndarray,\n",
    "                                   segment_length: int, overlap: int) -> List[Dict]:\n",
    "        \"\"\"Create overlapping segments.\"\"\"\n",
    "        segments = []\n",
    "        step = segment_length - overlap\n",
    "        \n",
    "        for i in range(0, len(years) - segment_length + 1, step):\n",
    "            end_idx = i + segment_length\n",
    "            segments.append({\n",
    "                'years': years[i:end_idx],\n",
    "                'emissions': emissions[i:end_idx],\n",
    "                'start_year': years[i],\n",
    "                'end_year': years[end_idx-1]\n",
    "            })\n",
    "            \n",
    "        return segments\n",
    "    \n",
    "    def _evaluate_segmentation(self, segments: List[Dict]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate segmentation quality based on pooled residual variance\n",
    "        and segment characteristics.\n",
    "        \"\"\"\n",
    "        all_residuals = []\n",
    "        total_points = 0\n",
    "        weighted_r2 = 0\n",
    "        \n",
    "        for segment in segments:\n",
    "            X = segment['years'].reshape(-1, 1)\n",
    "            y = segment['emissions']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            r2 = model.score(X, y)\n",
    "            \n",
    "            all_residuals.extend(residuals)\n",
    "            total_points += len(y)\n",
    "            weighted_r2 += r2 * len(y)\n",
    "        \n",
    "        pooled_variance = np.var(all_residuals)\n",
    "        avg_r2 = weighted_r2 / total_points\n",
    "        \n",
    "        # Score combines residual variance (lower is better) with R² penalty\n",
    "        # We want low residual variance but also reasonable fit quality\n",
    "        score = pooled_variance * (2 - avg_r2)  # Penalty increases if R² is low\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def analyze_autocorrelation(self, max_lags: int = 10) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze autocorrelation in residuals and determine appropriate\n",
    "        correction methods.\n",
    "        \"\"\"\n",
    "        if self.optimal_segments is None:\n",
    "            raise ValueError(\"Must optimize segments first\")\n",
    "        \n",
    "        # Calculate residuals from optimal segmentation\n",
    "        segments = self.optimal_segments['best']['segments']\n",
    "        all_residuals = []\n",
    "        residuals_with_years = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            X = segment['years'].reshape(-1, 1)\n",
    "            y = segment['emissions']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            \n",
    "            all_residuals.extend(residuals)\n",
    "            for i, year in enumerate(segment['years']):\n",
    "                residuals_with_years.append((year, residuals[i]))\n",
    "        \n",
    "        # Sort by year to ensure proper temporal order\n",
    "        residuals_with_years.sort()\n",
    "        temporal_residuals = np.array([r[1] for r in residuals_with_years])\n",
    "        \n",
    "        # Calculate autocorrelation function\n",
    "        acf_values = acf(temporal_residuals, nlags=max_lags, fft=True)\n",
    "        \n",
    "        # Ljung-Box test for serial correlation\n",
    "        lb_test = acorr_ljungbox(temporal_residuals, lags=max_lags, return_df=True)\n",
    "        \n",
    "        # Determine if significant autocorrelation exists\n",
    "        significant_lags = []\n",
    "        for lag in range(1, max_lags + 1):\n",
    "            if abs(acf_values[lag]) > 1.96 / np.sqrt(len(temporal_residuals)):\n",
    "                significant_lags.append(lag)\n",
    "        \n",
    "        self.autocorr_results = {\n",
    "            'acf_values': acf_values,\n",
    "            'ljung_box_test': lb_test,\n",
    "            'significant_lags': significant_lags,\n",
    "            'has_autocorrelation': len(significant_lags) > 0,\n",
    "            'temporal_residuals': temporal_residuals,\n",
    "            'max_autocorr_lag1': abs(acf_values[1]) if len(acf_values) > 1 else 0\n",
    "        }\n",
    "        \n",
    "        print(f\"Autocorrelation analysis complete:\")\n",
    "        print(f\"  Lag-1 autocorrelation: {acf_values[1]:.3f}\")\n",
    "        print(f\"  Significant lags: {significant_lags}\")\n",
    "        print(f\"  Has significant autocorrelation: {self.autocorr_results['has_autocorrelation']}\")\n",
    "        \n",
    "        return self.autocorr_results\n",
    "    \n",
    "    def characterize_noise(self, distribution: str = \"auto\", \n",
    "                          correct_autocorr: bool = True) -> \"EnhancedEmissionsPeakTest\":\n",
    "        \"\"\"\n",
    "        Characterize noise using optimal segments and autocorrelation correction.\n",
    "        \"\"\"\n",
    "        if self.optimal_segments is None:\n",
    "            self.optimize_segment_lengths()\n",
    "        \n",
    "        if self.autocorr_results is None:\n",
    "            self.analyze_autocorrelation()\n",
    "        \n",
    "        # Get residuals from optimal segmentation\n",
    "        segments = self.optimal_segments['best']['segments']\n",
    "        all_residuals = []\n",
    "        \n",
    "        for segment in segments:\n",
    "            X = segment['years'].reshape(-1, 1)\n",
    "            y = segment['emissions']\n",
    "            \n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            residuals = y - model.predict(X)\n",
    "            all_residuals.extend(residuals)\n",
    "        \n",
    "        self.residuals = pd.Series(all_residuals)\n",
    "        \n",
    "        # Apply autocorrelation correction if needed\n",
    "        if correct_autocorr and self.autocorr_results['has_autocorrelation']:\n",
    "            print(\"Applying autocorrelation correction...\")\n",
    "            effective_residuals = self._apply_autocorr_correction(self.residuals)\n",
    "        else:\n",
    "            effective_residuals = self.residuals\n",
    "        \n",
    "        # Fit noise distribution\n",
    "        self.noise_params, self.noise_generator = self._fit_noise_distribution(\n",
    "            effective_residuals, distribution\n",
    "        )\n",
    "        \n",
    "        print(\"Enhanced noise characterization complete:\")\n",
    "        print(f\"  Segments: {self.optimal_segments['best']['n_segments']}\")\n",
    "        print(f\"  Autocorrelation correction: {correct_autocorr and self.autocorr_results['has_autocorrelation']}\")\n",
    "        print(f\"  Noise std: {self.noise_params['sigma']:.1f} Mt\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _apply_autocorr_correction(self, residuals: pd.Series) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Apply autocorrelation correction by pre-whitening or effective sample size adjustment.\n",
    "        \"\"\"\n",
    "        # Method 1: Pre-whitening (removing AR(1) component)\n",
    "        if len(self.autocorr_results['significant_lags']) > 0:\n",
    "            lag1_corr = self.autocorr_results['acf_values'][1]\n",
    "            \n",
    "            # Simple AR(1) pre-whitening\n",
    "            temporal_residuals = self.autocorr_results['temporal_residuals']\n",
    "            prewhitened = temporal_residuals[1:] - lag1_corr * temporal_residuals[:-1]\n",
    "            \n",
    "            # Scale up variance to account for correlation\n",
    "            variance_inflation = 1 / (1 - lag1_corr**2)\n",
    "            corrected_residuals = prewhitened * np.sqrt(variance_inflation)\n",
    "            \n",
    "            return corrected_residuals\n",
    "        \n",
    "        return residuals.values\n",
    "    \n",
    "    def _fit_noise_distribution(self, residuals: np.ndarray, \n",
    "                               noise_type: str = \"auto\") -> Tuple[Dict, Callable]:\n",
    "        \"\"\"Fit noise distribution with autocorrelation considerations.\"\"\"\n",
    "        \n",
    "        if noise_type == \"auto\":\n",
    "            # Test both distributions\n",
    "            normal_params = stats.norm.fit(residuals)\n",
    "            t_params = stats.t.fit(residuals)\n",
    "            \n",
    "            normal_aic = -2 * np.sum(stats.norm.logpdf(residuals, *normal_params)) + 2 * 2\n",
    "            t_aic = -2 * np.sum(stats.t.logpdf(residuals, *t_params)) + 2 * 3\n",
    "            \n",
    "            noise_type = \"normal\" if normal_aic <= t_aic else \"t\"\n",
    "            print(f\"Auto-selected {noise_type} distribution\")\n",
    "        \n",
    "        if noise_type == \"normal\":\n",
    "            params_tuple = stats.norm.fit(residuals)\n",
    "            params = {\n",
    "                \"type\": \"normal\",\n",
    "                \"mu\": params_tuple[0],\n",
    "                \"sigma\": params_tuple[1],\n",
    "                \"scale\": params_tuple[1],\n",
    "                \"fitted_params\": params_tuple,\n",
    "            }\n",
    "            \n",
    "            def noise_generator(size):\n",
    "                return stats.norm.rvs(loc=params[\"mu\"], scale=params[\"sigma\"], size=size)\n",
    "        \n",
    "        elif noise_type == \"t\":\n",
    "            params_tuple = stats.t.fit(residuals)\n",
    "            params = {\n",
    "                \"type\": \"t\",\n",
    "                \"df\": params_tuple[0],\n",
    "                \"mu\": params_tuple[1],\n",
    "                \"scale\": params_tuple[2],\n",
    "                \"sigma\": params_tuple[2],\n",
    "                \"fitted_params\": params_tuple,\n",
    "            }\n",
    "            \n",
    "            def noise_generator(size):\n",
    "                return stats.t.rvs(df=params[\"df\"], loc=params[\"mu\"], \n",
    "                                 scale=params[\"scale\"], size=size)\n",
    "        \n",
    "        return params, noise_generator\n",
    "    \n",
    "    def set_test_data(self, test_data: List[Tuple[int, float]]) -> \"EnhancedEmissionsPeakTest\":\n",
    "        \"\"\"Set test data and calculate recent historical trend for null hypothesis.\"\"\"\n",
    "        self.test_data = pd.DataFrame(test_data, columns=[\"year\", \"emissions\"])\n",
    "        self.test_data = self.test_data.sort_values(\"year\").reset_index(drop=True)\n",
    "        \n",
    "        # Calculate test trend\n",
    "        X = self.test_data[\"year\"].values.reshape(-1, 1)\n",
    "        y = self.test_data[\"emissions\"].values\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        self.test_slope = model.coef_[0]\n",
    "        self.test_r2 = model.score(X, y)\n",
    "        \n",
    "        # Calculate recent historical trend for null hypothesis\n",
    "        self.recent_historical_trend = self._calculate_recent_historical_trend()\n",
    "        \n",
    "        print(f\"Test data set: {self.test_data['year'].min()}-{self.test_data['year'].max()}\")\n",
    "        print(f\"Test slope: {self.test_slope:.2f} Mt/year (R² = {self.test_r2:.3f})\")\n",
    "        print(f\"Recent historical trend: {self.recent_historical_trend:.2f} Mt/year\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _calculate_recent_historical_trend(self, n_recent_years: int = 10) -> float:\n",
    "        \"\"\"Calculate trend from recent historical data for null hypothesis.\"\"\"\n",
    "        recent_data = self.historical_data.tail(n_recent_years)\n",
    "        \n",
    "        X = recent_data[\"year\"].values.reshape(-1, 1)\n",
    "        y = recent_data[\"emissions\"].values\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        \n",
    "        return model.coef_[0]\n",
    "    \n",
    "    def run_enhanced_bootstrap_test(self, n_bootstrap: int = 10000, \n",
    "                                  null_hypothesis: str = \"recent_trend\",\n",
    "                                  alpha: float = 0.05) -> Dict:\n",
    "        \"\"\"\n",
    "        Run enhanced bootstrap test with choice of null hypothesis.\n",
    "        \n",
    "        Args:\n",
    "            null_hypothesis: \"zero_trend\" or \"recent_trend\"\n",
    "        \"\"\"\n",
    "        if self.test_data is None:\n",
    "            raise ValueError(\"Must set test data first\")\n",
    "        if self.noise_generator is None:\n",
    "            raise ValueError(\"Must characterize noise first\")\n",
    "        \n",
    "        print(f\"Running enhanced bootstrap test ({null_hypothesis} null hypothesis)...\")\n",
    "        \n",
    "        if null_hypothesis == \"recent_trend\":\n",
    "            bootstrap_slopes = self._generate_bootstrap_slopes_recent_trend(n_bootstrap)\n",
    "        else:  # zero_trend\n",
    "            bootstrap_slopes = self._generate_bootstrap_slopes_zero_trend(n_bootstrap)\n",
    "        \n",
    "        # Calculate p-values\n",
    "        p_value_one_tail = np.sum(bootstrap_slopes <= self.test_slope) / len(bootstrap_slopes)\n",
    "        \n",
    "        self.bootstrap_results = {\n",
    "            \"test_slope\": self.test_slope,\n",
    "            \"test_r2\": self.test_r2,\n",
    "            \"bootstrap_slopes\": bootstrap_slopes,\n",
    "            \"p_value_one_tail\": p_value_one_tail,\n",
    "            \"significant_one_tail\": p_value_one_tail < alpha,\n",
    "            \"alpha\": alpha,\n",
    "            \"n_bootstrap\": n_bootstrap,\n",
    "            \"null_hypothesis\": null_hypothesis,\n",
    "            \"recent_historical_trend\": getattr(self, 'recent_historical_trend', 0),\n",
    "            \"bootstrap_mean\": np.mean(bootstrap_slopes),\n",
    "            \"bootstrap_std\": np.std(bootstrap_slopes),\n",
    "        }\n",
    "        \n",
    "        print(f\"Enhanced bootstrap test complete:\")\n",
    "        print(f\"  Null hypothesis: {null_hypothesis}\")\n",
    "        print(f\"  P-value (one-tail): {p_value_one_tail:.4f}\")\n",
    "        print(f\"  Significant (α={alpha}): {self.bootstrap_results['significant_one_tail']}\")\n",
    "        \n",
    "        return self.bootstrap_results\n",
    "    \n",
    "    def _generate_bootstrap_slopes_recent_trend(self, n_bootstrap: int) -> np.ndarray:\n",
    "        \"\"\"Generate bootstrap slopes under recent historical trend null hypothesis.\"\"\"\n",
    "        bootstrap_slopes = []\n",
    "        n_points = len(self.test_data)\n",
    "        years = np.arange(2020, 2020 + n_points)\n",
    "        \n",
    "        # Use recent historical trend as baseline\n",
    "        baseline_trend = self.recent_historical_trend\n",
    "        baseline_emissions = np.mean(self.test_data[\"emissions\"])\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            # Generate null hypothesis data (recent trend + noise)\n",
    "            trend_emissions = baseline_emissions + baseline_trend * (years - years[0])\n",
    "            null_emissions = trend_emissions + self.noise_generator(n_points)\n",
    "            \n",
    "            # Calculate slope\n",
    "            X = years.reshape(-1, 1)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, null_emissions)\n",
    "            bootstrap_slopes.append(model.coef_[0])\n",
    "        \n",
    "        return np.array(bootstrap_slopes)\n",
    "    \n",
    "    def _generate_bootstrap_slopes_zero_trend(self, n_bootstrap: int) -> np.ndarray:\n",
    "        \"\"\"Generate bootstrap slopes under zero trend null hypothesis.\"\"\"\n",
    "        bootstrap_slopes = []\n",
    "        n_points = len(self.test_data)\n",
    "        years = np.arange(2020, 2020 + n_points)\n",
    "        baseline_emissions = np.mean(self.test_data[\"emissions\"])\n",
    "        \n",
    "        for _ in range(n_bootstrap):\n",
    "            null_emissions = np.full(n_points, baseline_emissions) + self.noise_generator(n_points)\n",
    "            \n",
    "            X = years.reshape(-1, 1)\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, null_emissions)\n",
    "            bootstrap_slopes.append(model.coef_[0])\n",
    "        \n",
    "        return np.array(bootstrap_slopes)\n",
    "    \n",
    "    def plot_enhanced_analysis(self, figsize: Tuple[int, int] = (16, 12)) -> plt.Figure:\n",
    "        \"\"\"Create comprehensive visualization of enhanced analysis.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 2, figsize=figsize)\n",
    "        \n",
    "        # 1. Historical data with optimal segments\n",
    "        self._plot_optimal_segments(axes[0, 0])\n",
    "        \n",
    "        # 2. Autocorrelation analysis\n",
    "        self._plot_autocorrelation(axes[0, 1])\n",
    "        \n",
    "        # 3. Noise distribution comparison\n",
    "        self._plot_noise_comparison(axes[1, 0])\n",
    "        \n",
    "        # 4. Bootstrap results with both null hypotheses\n",
    "        self._plot_enhanced_bootstrap_results(axes[1, 1])\n",
    "        \n",
    "        # 5. Segmentation comparison\n",
    "        self._plot_segmentation_comparison(axes[2, 0])\n",
    "        \n",
    "        # 6. Summary\n",
    "        self._plot_enhanced_summary(axes[2, 1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return fig\n",
    "    \n",
    "    def _plot_optimal_segments(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Plot historical data with optimal segments highlighted.\"\"\"\n",
    "        ax.plot(self.historical_data[\"year\"], self.historical_data[\"emissions\"], \n",
    "                'b-', alpha=0.7, label=\"Historical emissions\")\n",
    "        \n",
    "        # Overlay segments with different colors\n",
    "        colors = plt.cm.Set3(np.linspace(0, 1, len(self.optimal_segments['best']['segments'])))\n",
    "        \n",
    "        for i, (segment, color) in enumerate(zip(self.optimal_segments['best']['segments'], colors)):\n",
    "            X = segment['years'].reshape(-1, 1)\n",
    "            y = segment['emissions']\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            trend = model.predict(X)\n",
    "            \n",
    "            ax.plot(segment['years'], trend, color=color, linewidth=2, \n",
    "                   label=f\"Segment {i+1}\" if i < 3 else \"\")\n",
    "        \n",
    "        if hasattr(self, 'test_data') and self.test_data is not None:\n",
    "            ax.plot(self.test_data[\"year\"], self.test_data[\"emissions\"], \n",
    "                   'ro', markersize=8, label=\"Test data\")\n",
    "        \n",
    "        ax.set_xlabel(\"Year\")\n",
    "        ax.set_ylabel(\"CO₂ Emissions (Mt)\")\n",
    "        ax.set_title(f\"Optimal Segmentation ({self.optimal_segments['best']['method']})\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_autocorrelation(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Plot autocorrelation function.\"\"\"\n",
    "        if self.autocorr_results is None:\n",
    "            ax.text(0.5, 0.5, \"No autocorrelation analysis\", ha='center', va='center', \n",
    "                   transform=ax.transAxes)\n",
    "            return\n",
    "        \n",
    "        lags = range(len(self.autocorr_results['acf_values']))\n",
    "        ax.plot(lags, self.autocorr_results['acf_values'], 'bo-')\n",
    "        \n",
    "        # Add confidence intervals\n",
    "        n = len(self.autocorr_results['temporal_residuals'])\n",
    "        conf_interval = 1.96 / np.sqrt(n)\n",
    "        ax.axhline(conf_interval, color='red', linestyle='--', alpha=0.7)\n",
    "        ax.axhline(-conf_interval, color='red', linestyle='--', alpha=0.7)\n",
    "        ax.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "        \n",
    "        ax.set_xlabel(\"Lag\")\n",
    "        ax.set_ylabel(\"Autocorrelation\")\n",
    "        ax.set_title(\"Autocorrelation Function of Residuals\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_noise_comparison(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Plot noise distribution.\"\"\"\n",
    "        ax.hist(self.residuals, bins=30, density=True, alpha=0.7, \n",
    "               color=\"skyblue\", label=\"Residuals\")\n",
    "        \n",
    "        if self.noise_params[\"type\"] == \"normal\":\n",
    "            x_range = np.linspace(self.residuals.min(), self.residuals.max(), 100)\n",
    "            fitted_density = stats.norm.pdf(x_range, self.noise_params[\"mu\"], \n",
    "                                          self.noise_params[\"sigma\"])\n",
    "            ax.plot(x_range, fitted_density, \"r-\", linewidth=2, label=\"Fitted distribution\")\n",
    "        \n",
    "        ax.set_xlabel(\"Residuals (Mt)\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(f'Enhanced Noise Distribution (σ = {self.noise_params[\"sigma\"]:.1f} Mt)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_enhanced_bootstrap_results(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Plot bootstrap results.\"\"\"\n",
    "        if self.bootstrap_results is None:\n",
    "            return\n",
    "            \n",
    "        ax.hist(self.bootstrap_results[\"bootstrap_slopes\"], bins=50, density=True, \n",
    "               alpha=0.7, color=\"lightgreen\", \n",
    "               label=f'Bootstrap slopes\\n({self.bootstrap_results[\"null_hypothesis\"]} null)')\n",
    "        \n",
    "        ax.axvline(self.bootstrap_results[\"test_slope\"], color=\"red\", linewidth=2,\n",
    "                  label=f'Observed slope\\n{self.bootstrap_results[\"test_slope\"]:.1f} Mt/yr')\n",
    "        \n",
    "        if self.bootstrap_results[\"null_hypothesis\"] == \"recent_trend\":\n",
    "            ax.axvline(self.bootstrap_results[\"recent_historical_trend\"], \n",
    "                      color=\"orange\", linewidth=2, linestyle='--',\n",
    "                      label=f'Recent historical trend\\n{self.bootstrap_results[\"recent_historical_trend\"]:.1f} Mt/yr')\n",
    "        \n",
    "        ax.set_xlabel(\"Slope (Mt/year)\")\n",
    "        ax.set_ylabel(\"Density\")\n",
    "        ax.set_title(\"Enhanced Bootstrap Distribution\")\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_segmentation_comparison(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Compare different segmentation methods.\"\"\"\n",
    "        if self.optimal_segments is None:\n",
    "            return\n",
    "            \n",
    "        methods = []\n",
    "        scores = []\n",
    "        n_segments = []\n",
    "        \n",
    "        for seg in self.optimal_segments['all_tested']:\n",
    "            methods.append(f\"{seg['method']}\\n(n={seg['n_segments']})\")\n",
    "            scores.append(seg['score'])\n",
    "            n_segments.append(seg['n_segments'])\n",
    "        \n",
    "        bars = ax.bar(methods, scores)\n",
    "        \n",
    "        # Highlight the best method\n",
    "        best_idx = np.argmin(scores)\n",
    "        bars[best_idx].set_color('red')\n",
    "        bars[best_idx].set_alpha(0.8)\n",
    "        \n",
    "        ax.set_ylabel(\"Score (lower is better)\")\n",
    "        ax.set_title(\"Segmentation Method Comparison\")\n",
    "        ax.tick_params(axis='x', rotation=45)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    def _plot_enhanced_summary(self, ax: plt.Axes) -> None:\n",
    "        \"\"\"Plot enhanced summary statistics.\"\"\"\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "        if self.bootstrap_results is None:\n",
    "            return\n",
    "        \n",
    "        summary_text = f\"\"\"\n",
    "        ENHANCED STATISTICAL TEST RESULTS\n",
    "        _________________________________\n",
    "        \n",
    "        Segmentation:\n",
    "        • Method: {self.optimal_segments['best']['method']}\n",
    "        • Segments: {self.optimal_segments['best']['n_segments']}\n",
    "        \n",
    "        Autocorrelation:\n",
    "        • Lag-1 correlation: {self.autocorr_results['acf_values'][1]:.3f}\n",
    "        • Significant lags: {len(self.autocorr_results['significant_lags'])}\n",
    "        \n",
    "        Test Results:\n",
    "        • Observed slope: {self.bootstrap_results['test_slope']:.2f} Mt/year\n",
    "        • Null hypothesis: {self.bootstrap_results['null_hypothesis']}\n",
    "        • P-value: {self.bootstrap_results['p_value_one_tail']:.4f}\n",
    "        • Significant: {self.bootstrap_results['significant_one_tail']}\n",
    "        \n",
    "        Noise Model:\n",
    "        • Distribution: {self.noise_params['type']}\n",
    "        • Std deviation: {self.noise_params['sigma']:.1f} Mt\n",
    "        \n",
    "        CONCLUSION:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add conclusion based on results\n",
    "        if self.bootstrap_results['significant_one_tail'] and self.bootstrap_results['test_slope'] < 0:\n",
    "            if self.bootstrap_results['null_hypothesis'] == 'recent_trend':\n",
    "                conclusion = \"Strong evidence of acceleration in emissions decline\"\n",
    "            else:\n",
    "                conclusion = \"Strong evidence that emissions have peaked\"\n",
    "        elif self.bootstrap_results['test_slope'] < 0:\n",
    "            conclusion = \"Declining trend but not statistically significant\"\n",
    "        else:\n",
    "            conclusion = \"No evidence of emissions peak\"\n",
    "        \n",
    "        summary_text += conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdbf918-26ac-4060-87c1-089fe2ea1ce7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
